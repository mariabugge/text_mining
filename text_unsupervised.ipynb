{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual data: unsupervised algorithms (clustering)\n",
    "\n",
    "\n",
    "### Data: The 20 newsgroup dataset from sklearn.datasets\n",
    "The dataset contains (labeled) news with different topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(subset='all', shuffle=True, random_state=100, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18846 documents\n",
      "With 20 categories\n",
      "First five label names ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware']\n",
      "The true k is 20\n"
     ]
    }
   ],
   "source": [
    "#Exploring data structure\n",
    "print(\"There are %d documents\" % len(dataset.data))\n",
    "print(\"With %d categories\" % len(dataset.target_names))\n",
    "print(\"First five label names %s\" % dataset.target_names[:5])\n",
    "labels = dataset.target #our Y (our outcome)\n",
    "true_k = np.unique(labels).shape[0] #true number of different labels\n",
    "print(\"The true k is %d\" % true_k)\n",
    "#dataset[\"data\"][0] to print first news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are 1000 features\n",
    "n_features=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's start by \n",
    "vectorizer = TfidfVectorizer(max_df=0.5, #document must appear in max 50% of the documents\n",
    "                             max_features=n_features, #max features = vocabulary size = 1000\n",
    "                             min_df=2, #must appear in at least 2 doc\n",
    "                             stop_words='english', #remove stopwords\n",
    "                             use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 18846, n_features: 1000\n"
     ]
    }
   ],
   "source": [
    "#fitting and transforming the data with the vectorizer we just specified\n",
    "X = vectorizer.fit_transform(dataset.data)\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K means (MiniBatch Kmeans version)\n",
    "\n",
    "It is a more efficient version of the kmeans algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniBatchKMeans(batch_size=100, compute_labels=True, init='k-means++',\n",
       "        init_size=None, max_iter=100, max_no_improvement=10, n_clusters=20,\n",
       "        n_init=3, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
       "        verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km = MiniBatchKMeans(n_clusters=true_k)\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: use using windows need does server don know like problem\n",
      "Cluster 1: god jesus christ christians believe faith bible sin people christian\n",
      "Cluster 2: known note non long used time end edu effect email\n",
      "Cluster 3: card video drivers cards monitor driver windows vga know does\n",
      "Cluster 4: book books read edu good time people source reading called\n",
      "Cluster 5: 00 sale 50 15 10 20 11 30 shipping list\n",
      "Cluster 6: government disease medical patients market people health federal cancer insurance\n",
      "Cluster 7: read bible church does note people know believe don greek\n",
      "Cluster 8: key chip encryption clipper keys government algorithm public des security\n",
      "Cluster 9: thanks advance does know hi windows mail anybody program help\n",
      "Cluster 10: armenian armenians turkish turkey armenia genocide people russian muslim soviet\n",
      "Cluster 11: windows dos scsi mac modem port pc bus serial file\n",
      "Cluster 12: ve bike know seen like don just good got new\n",
      "Cluster 13: com away said ibm mail sun hp offer does edu\n",
      "Cluster 14: drive drives disk hard scsi floppy cd apple controller tape\n",
      "Cluster 15: israel israeli arab jews jewish peace rights people american right\n",
      "Cluster 16: edu new just does like car used know mail good\n",
      "Cluster 17: don people think just like know time right good really\n",
      "Cluster 18: did just didn like know got think say fbi way\n",
      "Cluster 19: 10 water 15 problem 12 11 25 21 used 17\n"
     ]
    }
   ],
   "source": [
    "#Let's look at what the clusters look like\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic extraction clustering (NMF and LSA algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reloading dataset\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, \n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "\n",
    "n_features = 1000\n",
    "n_topics = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA\n",
    "\n",
    "LDA is based on frequency of terms, so we will use the CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(max_df=0.5, min_df=2, max_features=n_features, \n",
    "                                stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying model\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, \n",
    "                                learning_method=\"batch\").fit(tf)\n",
    "lda_W = lda.transform(tf)\n",
    "lda_H = lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to display the 'top10 words' in each cluster, ie those who represent the cluster the most.\n",
    "\n",
    "def display_topics_full(H, feature_names, documents, no_top_words):\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "key encryption chip keys clipper government security law use des\n",
      "Topic 1:\n",
      "car just bike like right cars engine speed dod good\n",
      "Topic 2:\n",
      "drive card scsi disk video bit hard drives pc bus\n",
      "Topic 3:\n",
      "00 1993 use number 10 health title 50 control page\n",
      "Topic 4:\n",
      "people mr gun think government don right state president know\n",
      "Topic 5:\n",
      "people said armenian armenians turkish did didn went children know\n",
      "Topic 6:\n",
      "price new good sale offer mail sell interested buy like\n",
      "Topic 7:\n",
      "cx ah w7 chz 34u lk pl air jews 17\n",
      "Topic 8:\n",
      "game team year games play good season think players win\n",
      "Topic 9:\n",
      "thanks know does help problem like hi just advance use\n",
      "Topic 10:\n",
      "available window edu server version graphics use motif application software\n",
      "Topic 11:\n",
      "edu com mail internet send list information email anonymous address\n",
      "Topic 12:\n",
      "god jesus people does believe say bible christian think life\n",
      "Topic 13:\n",
      "don just like think time know good use people ve\n",
      "Topic 14:\n",
      "space nasa data launch science earth research program satellite center\n",
      "Topic 15:\n",
      "new university 000 april american york national san year city\n",
      "Topic 16:\n",
      "db windows memory dos driver tape mouse drivers using running\n",
      "Topic 17:\n",
      "10 25 11 15 12 16 14 20 13 18\n",
      "Topic 18:\n",
      "file files israel program output entry image israeli windows jpeg\n",
      "Topic 19:\n",
      "ax max g9v b8f a86 pl 145 1d9 1t 0t\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 10\n",
    "n_top_documents = 10\n",
    "\n",
    "display_topics_full(lda_H,  tf_feature_names, documents, \n",
    "                    n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF\n",
    "\n",
    "With NMF we will use the TD IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, max_features=n_features, \n",
    "                                   stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying model\n",
    "nmf = NMF(n_components=n_topics).fit(tfidf)\n",
    "nmf_W = nmf.transform(tfidf)\n",
    "nmf_H = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "people government right armenian gun armenians law said turkish rights\n",
      "Topic 1:\n",
      "card video monitor cards drivers bus vga driver color memory\n",
      "Topic 2:\n",
      "god jesus bible believe christ faith christian christians church life\n",
      "Topic 3:\n",
      "game team year games season players play hockey win league\n",
      "Topic 4:\n",
      "new 00 sale 10 price offer shipping condition 20 50\n",
      "Topic 5:\n",
      "thanks mail advance hi looking info information address help email\n",
      "Topic 6:\n",
      "windows file files use dos window program using problem running\n",
      "Topic 7:\n",
      "edu soon university cs ftp email article internet david pub\n",
      "Topic 8:\n",
      "key chip encryption clipper keys use government escrow public algorithm\n",
      "Topic 9:\n",
      "drive scsi hard drives disk ide floppy controller cd mac\n",
      "Topic 10:\n",
      "just thought tell ll mean oh little wanted work maybe\n",
      "Topic 11:\n",
      "does anybody mean say work exist actually make doesn help\n",
      "Topic 12:\n",
      "think don say mean try want better really believe case\n",
      "Topic 13:\n",
      "like sounds looks look things lot sound use make doesn\n",
      "Topic 14:\n",
      "know don let need anybody want maybe doesn sure didn\n",
      "Topic 15:\n",
      "car cars engine speed driver bike buy insurance price power\n",
      "Topic 16:\n",
      "good ve time got did really ll going problem years\n",
      "Topic 17:\n",
      "israel israeli jews jewish war state peace did anti land\n",
      "Topic 18:\n",
      "space nasa shuttle launch data orbit earth program station science\n",
      "Topic 19:\n",
      "com list dave article internet sun address phone email hp\n"
     ]
    }
   ],
   "source": [
    "#Displaying top 10 words for each cluster, with same function as previously\n",
    "n_top_words = 10\n",
    "n_top_documents = 10\n",
    "display_topics_full(nmf_H,  tfidf_feature_names, documents, \n",
    "                    n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
