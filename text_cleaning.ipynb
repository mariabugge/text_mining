{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual data analysis\n",
    "\n",
    "### Data: Tweet samples downloaded from the nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gensim\n",
    "! pip install nltk --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set up\n",
    "\n",
    "import nltk\n",
    "\n",
    "#RUN ONLY ONCE. When running the nltk.download() command a new window will open, \n",
    "#click the 'corpora' tab and download stopwords and twitter_samples \n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import remaining libraries\n",
    "\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.collocations import *\n",
    "\n",
    "from nltk import tokenize\n",
    "\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "There are three json files in twitter_samples. I will work on positive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = twitter_samples.strings('positive_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)', '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!', '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!']\n"
     ]
    }
   ],
   "source": [
    "#Preview of the first four tweets\n",
    "print(tweets[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mariabugge/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "#To tokenize the tweets (ie cut the tweets into words) we need to download 'punkt' from the nltk library\n",
    "nltk.download('punkt')\n",
    "tokens = [tokenize.word_tokenize(s) for s in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@', 'DespiteOfficial', 'we', 'had', 'a', 'listen', 'last', 'night', ':', ')', 'As', 'You', 'Bleed', 'is', 'an', 'amazing', 'track', '.', 'When', 'are', 'you', 'in', 'Scotland', '?', '!']\n"
     ]
    }
   ],
   "source": [
    "#Let's look at how our third tweet: \n",
    "print(tokens[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now analyze words instead of sentences. As a start, let's create a pandas series in which we'll have each word and how many times it appears in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_words = pd.Series()\n",
    "for s in tokens :\n",
    "    count_words = count_words.append(pd.Series(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":      6667\n",
       ")      5165\n",
       "@      5119\n",
       "!      1920\n",
       "you    1427\n",
       ".      1323\n",
       "#      1292\n",
       "I      1177\n",
       "to     1063\n",
       "the     997\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Printing the top 10 most used words\n",
    "count_words.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the most common words are punctuation or stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mariabugge/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@         2454\n",
      "http       851\n",
      "D          611\n",
      "https      330\n",
      "Thanks     203\n",
      "day        199\n",
      "thanks     176\n",
      "amp        165\n",
      "i          142\n",
      "Hi         140\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Let's redo the previous pandas series, but this time taking into account the category of the word (noun, verb,...) and counting \n",
    "#only nouns\n",
    "nltk.download('averaged_perceptron_tagger') #needed to use the pos_tag function, which gives the category\n",
    "word_cat = [nltk.pos_tag(s) for s in tokens]\n",
    "\n",
    "nouns = pd.Series()\n",
    "for sent in word_cat :\n",
    "    nouns = nouns.append(pd.Series([w[0] for w in sent if \"NN\" in w[1]])) #NN = nouns \n",
    "print(nouns.value_counts()[:10])    #printing our top10 again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better! But still some noisy words. Moving on, we will clean the data. When cleaning textual data, we remove punctuation and lowercase all words\n",
    "\n",
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_no_punct_lower = []\n",
    "for s in tokens  : \n",
    "    tokens_no_punct_lower.append([w.lower() for w in s if w.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['despiteofficial',\n",
       " 'we',\n",
       " 'had',\n",
       " 'a',\n",
       " 'listen',\n",
       " 'last',\n",
       " 'night',\n",
       " 'as',\n",
       " 'you',\n",
       " 'bleed',\n",
       " 'is',\n",
       " 'an',\n",
       " 'amazing',\n",
       " 'track',\n",
       " 'when',\n",
       " 'are',\n",
       " 'you',\n",
       " 'in',\n",
       " 'scotland']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_no_punct_lower[2] #what our tweet looks like now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step, removing stopwords. Here, the tweets are in english, so we specify the english language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "#as the data is tweets, we will add some twitter-language-based words too, such as 'RT' or 'http'\n",
    "stop_words.extend([\"rt\",\"http\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's remove the stopwords from our tweets\n",
    "tokens_wo_stop_word = []\n",
    "for s in tokens_no_punct_lower  : \n",
    "    tokens_wo_stop_word.append([w for w in s if w not in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming (ie considering roots of words instead of whole word)\n",
    "\n",
    "This allow for the algorithm to consider the singular and plural version to be the same word for instance, and not count everything twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed = []\n",
    "porter = PorterStemmer() #using the PorterStemmer function to stem words. \n",
    "for s in tokens_wo_stop_word : \n",
    "    stemmed.append([porter.stem(w) for w in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "thank     642\n",
       "follow    446\n",
       "love      399\n",
       "http      336\n",
       "u         247\n",
       "day       241\n",
       "good      238\n",
       "like      232\n",
       "happi     209\n",
       "get       209\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We will redo the pandas series again and see what our top 10 looks like now:\n",
    "wordcount_Stemmed = pd.Series()\n",
    "for s in stemmed :\n",
    "    wordcount_Stemmed = wordcount_Stemmed.append(pd.Series(s))\n",
    "\n",
    "wordcount_Stemmed.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word associations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the Word2Vec method on our clean twitter sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Word2Vec(tokens_wo_stop_word) #our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the top 10 words most likely to be related to 'twitter' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('like', 0.9992236495018005),\n",
       " ('https', 0.9991927146911621),\n",
       " ('na', 0.9991474151611328),\n",
       " ('time', 0.9991419315338135),\n",
       " ('even', 0.9990895986557007),\n",
       " ('thanks', 0.9990859031677246),\n",
       " ('know', 0.9990695714950562),\n",
       " ('get', 0.9990658760070801),\n",
       " ('come', 0.9990612864494324),\n",
       " ('say', 0.9990571141242981)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.wv.most_similar(positive=[\"twitter\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the top10 least likely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fback', 0.08321775496006012),\n",
       " ('pleasse', -0.30082178115844727),\n",
       " ('sore', -0.8252904415130615),\n",
       " ('fridays', -0.8295408487319946),\n",
       " ('vikkfollows', -0.8992244005203247),\n",
       " ('jnlazts', -0.9073284864425659),\n",
       " ('mood', -0.9141058921813965),\n",
       " ('exactly', -0.9183617234230042),\n",
       " ('bae', -0.9233916401863098),\n",
       " ('belated', -0.9260453581809998)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.wv.most_similar(negative=[\"twitter\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
